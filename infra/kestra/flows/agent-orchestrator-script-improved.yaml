id: agent-orchestrator-script
namespace: default

description: |
  R2D2 AI Agent - Autonomous repository health monitoring with AI-powered decision-making.
  Uses OpenAI GPT-4o-mini to analyze GitHub metrics and intelligently decide appropriate actions.
  
  Workflow:
  1. Fetch GitHub repository metrics (issues, PRs, commits, activity)
  2. Calculate health score (0-100) using custom algorithm
  3. Use OpenAI to analyze metrics and recommend action
  4. Execute autonomous action via Cline CLI integration
  5. Post results to Next.js dashboard

tasks:
  - id: fetch_metrics
    type: io.kestra.plugin.scripts.node.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    description: Fetch GitHub repository metrics for AI analysis
    script: |-
      (async () => {
        try {
          const token = '{{ secret("GITHUB_TOKEN") }}';
          if (!token) { 
            console.log('NO_TOKEN_AVAILABLE'); 
            return; 
          }

          const headers = { 
            Authorization: 'token ' + token, 
            'User-Agent': 'kestra-r2d2-agent' 
          };

          console.log('Fetching repository metrics...');

          // Fetch repository metadata
          const repoRes = await fetch('https://api.github.com/repos/something1703/r2d2-agent', { headers });
          if (!repoRes.ok) { 
            console.error('REPO_FETCH_ERROR', repoRes.status); 
            return; 
          }
          const repo = await repoRes.json();

          // Fetch open issues
          const issuesRes = await fetch('https://api.github.com/repos/something1703/r2d2-agent/issues?state=open&per_page=20', { headers });
          if (!issuesRes.ok) { 
            console.error('ISSUES_FETCH_ERROR', issuesRes.status); 
            return; 
          }
          const issues = await issuesRes.json();

          // Fetch open pull requests
          const prsRes = await fetch('https://api.github.com/repos/something1703/r2d2-agent/pulls?state=open', { headers });
          const prs = prsRes.ok ? await prsRes.json() : [];

          // Fetch recent commits (last 7 days)
          const weekAgo = new Date();
          weekAgo.setDate(weekAgo.getDate() - 7);
          const commitsRes = await fetch(
            'https://api.github.com/repos/something1703/r2d2-agent/commits?since=' + weekAgo.toISOString(), 
            { headers }
          );
          const commits = commitsRes.ok ? await commitsRes.json() : [];

          console.log('Repository metrics collected successfully');

          // Calculate repository health score (0-100)
          let healthScore = 100;
          const issues_count = Array.isArray(issues) ? issues.length : 0;
          const prs_count = Array.isArray(prs) ? prs.length : 0;
          const commits_count = Array.isArray(commits) ? commits.length : 0;
          
          const lastUpdate = new Date(repo.updated_at);
          const daysSinceUpdate = Math.floor((Date.now() - lastUpdate) / (1000 * 60 * 60 * 24));

          // Health score calculation algorithm
          if (issues_count > 10) healthScore -= 30;
          else if (issues_count > 5) healthScore -= 15;
          else if (issues_count > 0) healthScore -= 5;
          
          if (daysSinceUpdate > 14) healthScore -= 40;
          else if (daysSinceUpdate > 7) healthScore -= 20;
          
          if (prs_count > 5) healthScore -= 25;
          else if (prs_count > 2) healthScore -= 10;
          else if (prs_count > 0) healthScore -= 5;
          
          if (commits_count === 0) healthScore -= 20;
          else if (commits_count > 20) healthScore += 10;
          
          if (repo.stargazers_count > 10) healthScore += 5;

          healthScore = Math.max(0, Math.min(100, healthScore));

          // Store metrics for AI analysis
          console.log('METRICS_OUTPUT::START');
          console.log(JSON.stringify({
            healthScore,
            issues_count,
            prs_count,
            commits_count,
            daysSinceUpdate,
            stars: repo.stargazers_count || 0,
            forks: repo.forks_count || 0,
            issue_titles: Array.isArray(issues) ? issues.slice(0, 5).map(i => i.title) : [],
            pr_titles: Array.isArray(prs) ? prs.slice(0, 3).map(p => p.title) : []
          }));
          console.log('METRICS_OUTPUT::END');

        } catch (e) {
          console.error('WORKFLOW ERROR:', e && e.stack ? e.stack : e);
          process.exit(1);
        }
      })();
    outputs:
      - id: metrics
        type: JSON
        value: "{{ outputs.fetch_metrics.stdout | jq('.') }}"

  - id: ai_decision
    type: io.kestra.plugin.scripts.node.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    description: Use OpenAI GPT-4o-mini to analyze metrics and recommend action
    script: |-
      (async () => {
        try {
          const openaiKey = '{{ secret("OPENAI_API_KEY") }}';
          if (!openaiKey) {
            console.log('NO_OPENAI_KEY - Falling back to rule-based logic');
            // Fallback logic would go here
            return;
          }

          // Extract metrics from previous task
          const metricsOutput = `{{ outputs.fetch_metrics.stdout }}`;
          const metricsMatch = metricsOutput.match(/METRICS_OUTPUT::START\n(.*)\nMETRICS_OUTPUT::END/s);
          
          if (!metricsMatch) {
            console.error('Could not parse metrics');
            return;
          }

          const metrics = JSON.parse(metricsMatch[1]);
          
          console.log('═══════════════════════════════════════');
          console.log('AI DECISION ENGINE - OpenAI GPT-4o-mini');
          console.log('═══════════════════════════════════════');
          console.log('Health Score:', metrics.healthScore + '/100');
          console.log('Open Issues:', metrics.issues_count);
          console.log('Open PRs:', metrics.prs_count);
          console.log('Recent Commits (7d):', metrics.commits_count);
          console.log('Days Since Update:', metrics.daysSinceUpdate);
          console.log('───────────────────────────────────────');

          // Prepare AI prompt with repository context
          const aiPrompt = `You are an expert DevOps AI agent analyzing a GitHub repository's health.

Repository Metrics:
- Health Score: ${metrics.healthScore}/100
- Open Issues: ${metrics.issues_count}
- Open Pull Requests: ${metrics.prs_count}
- Recent Commits (7 days): ${metrics.commits_count}
- Days Since Last Update: ${metrics.daysSinceUpdate}
- Stars: ${metrics.stars}
- Forks: ${metrics.forks}

Recent Issue Titles:
${metrics.issue_titles.map((t, i) => `${i + 1}. ${t}`).join('\n')}

Recent PR Titles:
${metrics.pr_titles.map((t, i) => `${i + 1}. ${t}`).join('\n')}

Based on these metrics, recommend ONE action from:
1. "fix-issues" - Critical issue backlog needs attention
2. "code-review" - Pull requests need review or recent code needs quality check
3. "update-docs" - Repository inactive, documentation refresh needed
4. "none" - Repository is healthy, no action required

Respond ONLY with valid JSON in this exact format:
{
  "action": "fix-issues|code-review|update-docs|none",
  "priority": "high|medium|low|none",
  "reasoning": "Brief explanation (max 100 chars)",
  "confidence": 0.0-1.0
}`;

          // Call OpenAI API
          const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${openaiKey}`
            },
            body: JSON.stringify({
              model: 'gpt-4o-mini',
              messages: [
                {
                  role: 'system',
                  content: 'You are an expert DevOps AI that analyzes repository health and recommends autonomous actions. Always respond with valid JSON only.'
                },
                {
                  role: 'user',
                  content: aiPrompt
                }
              ],
              temperature: 0.3,
              max_tokens: 150,
              response_format: { type: 'json_object' }
            })
          });

          if (!response.ok) {
            const error = await response.text();
            console.error('OpenAI API Error:', response.status, error);
            return;
          }

          const aiResponse = await response.json();
          const decision = JSON.parse(aiResponse.choices[0].message.content);

          console.log('AI DECISION RECEIVED:');
          console.log('Action:', decision.action);
          console.log('Priority:', decision.priority);
          console.log('Reasoning:', decision.reasoning);
          console.log('Confidence:', (decision.confidence * 100).toFixed(1) + '%');
          console.log('═══════════════════════════════════════');

          console.log('AI DECISION RECEIVED:');
          console.log('Action:', decision.action);
          console.log('Priority:', decision.priority);
          console.log('Reasoning:', decision.reasoning);
          console.log('Confidence:', (decision.confidence * 100).toFixed(1) + '%');
          console.log('═══════════════════════════════════════');

          // Store decision for next task
          console.log('DECISION_OUTPUT::START');
          console.log(JSON.stringify({
            action: decision.action,
            priority: decision.priority,
            reasoning: decision.reasoning,
            confidence: decision.confidence,
            timestamp: new Date().toISOString(),
            metrics: metrics
          }));
          console.log('DECISION_OUTPUT::END');

        } catch (e) {
          console.error('AI DECISION ERROR:', e && e.stack ? e.stack : e);
          process.exit(1);
        }
      })();

  - id: execute_action
    type: io.kestra.plugin.scripts.node.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    description: Execute autonomous action based on AI decision
    script: |-
      (async () => {
        try {
          // Parse AI decision from previous task
          const decisionOutput = `{{ outputs.ai_decision.stdout }}`;
          const decisionMatch = decisionOutput.match(/DECISION_OUTPUT::START\n(.*)\nDECISION_OUTPUT::END/s);
          
          if (!decisionMatch) {
            console.error('Could not parse AI decision');
            return;
          }

          const decisionData = JSON.parse(decisionMatch[1]);
          const { action, priority, reasoning, confidence, metrics } = decisionData;

          // Prepare summary for dashboard
          const summary = {
            repo: 'something1703/r2d2-agent',
            health_score: metrics.healthScore,
            metrics: {
              open_issues: metrics.issues_count,
              open_prs: metrics.prs_count,
              recent_commits: metrics.commits_count,
              days_since_update: metrics.daysSinceUpdate,
              stars: metrics.stars,
              forks: metrics.forks
            },
            issue_titles: metrics.issue_titles,
            decision: {
              action: action,
              priority: priority,
              reasoning: reasoning,
              confidence: confidence,
              timestamp: decisionData.timestamp,
              ai_powered: true
            },
            analyzed_at: new Date().toISOString()
          };

          console.log('Posting summary to Next.js dashboard...');

          // Post summary to Next.js API
          try {
            const host = process.env.NEXT_HOST || '172.17.0.1';
            const summaryRes = await fetch(`http://${host}:3000/api/kestra-summary`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify(summary)
            });
            
            if (summaryRes.ok) {
              console.log('✅ Summary posted to dashboard');
            } else {
              console.log('⚠️  Summary POST failed:', summaryRes.status);
            }
          } catch (e) {
            console.log('⚠️  Could not reach Next.js API:', e.message);
          }

          // Execute autonomous action (prevent infinite loops)
          if (action !== 'none' && action !== 'trigger-kestra') {
            try {
              const host = process.env.NEXT_HOST || '172.17.0.1';
              
              const payload = {
                action: action,
                priority: priority,
                context: {
                  healthScore: metrics.healthScore,
                  openIssues: metrics.issues_count,
                  openPRs: metrics.prs_count,
                  reasoning: reasoning,
                  aiConfidence: confidence
                }
              };
              
              console.log(`Triggering autonomous action: ${action} (priority: ${priority})`);
              
              const actionRes = await fetch(`http://${host}:3000/api/run-agent`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
              });
              
              if (actionRes.ok) {
                const result = await actionRes.json();
                console.log('✅ Action executed:', result.ok ? 'Success' : 'Failed');
              } else {
                console.log('⚠️  Action trigger failed:', actionRes.status);
              }
            } catch (e) {
              console.log('⚠️  Could not trigger action:', e.message);
            }
          } else {
            console.log('ℹ️  No action needed - repository healthy');
          }

          console.log('');
          console.log('═══════════════════════════════════════');
          console.log('WORKFLOW COMPLETED SUCCESSFULLY');
          console.log('AI-Powered Decision: ' + action);
          console.log('Confidence: ' + (confidence * 100).toFixed(1) + '%');
          console.log('═══════════════════════════════════════');

        } catch (e) {
          console.error('EXECUTION ERROR:', e && e.stack ? e.stack : e);
          process.exit(1);
        }
      })();

triggers:
  - id: schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 */6 * * *"  # Every 6 hours
    description: Autonomous monitoring - runs 4 times daily
